# visual-search-vlm
This project implements a visual search system using OpenAI's CLIP model. It lets users input a natural language query (e.g., "a cat on a sofa") and retrieves the most relevant images from a dataset based on semantic similarity. A simple Streamlit app is included for easy interaction
